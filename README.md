# llm-playground

["A Survey of Large Language Models"](https://arxiv.org/abs/2303.18223)

["Language Models are Few-Shot Learners"](https://arxiv.org/abs/2005.14165)

["Spread Your Wings: Falcon 180B is here"](https://huggingface.co/blog/falcon-180b)

["Cleaned Alpaca Dataset"](https://github.com/gururise/AlpacaDataCleaned)

["The Pile: An 800GB Dataset of Diverse Text for Language Modeling"](https://arxiv.org/abs/2101.00027)

["Training Compute-Optimal Large Language Models"](https://arxiv.org/abs/2203.15556)

["Train With Mixed Precision"](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html)

["Dropout: A Simple Way to Prevent Neural Networks from Overfitting"](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)

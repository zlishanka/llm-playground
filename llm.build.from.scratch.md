## Build LLM from scratch

### Costs of training 
- Llama2(7b) 	180,000 GPU hours
- Llama2(70b)	1,700,000 GPU hours 

### Key steps

#### Data Curation

- The quality of your model is driven by the quality of your data
- `GPT3 (175b)`: 0.5 T tokens, `Llama 2 (70b)`: 2T tokens `Falcon (180b)`:: 3.5 T tokens
- Where to get all data
	- Web pages, wikipedia, forums, books, scientific articles, code bases, etc.
- Public datasets
	- Common Crowl (Colossal Clean Crawled Corpus i.e. C4, Falcon RefinedWeb)
	- The pile
	- Hugging face datasets
- Private datasets
	- e.g. FinPile (BloombergGPT) drawn from Bloomberg archives
- Use an LLM to generate training data
	- e.g. Alpaca - an LLM trained on structured text generated by GPT-3
- How to prepare the data
	- Quality Filtering - remove "low quality" text 
		- classifier-based
		- heuristic-based
	- De-duplication
		- Serveral instances of same text (or very similar) can bias model and disrupt training
	- Privacy Redaction
		- Removal of sensitive and confidential information
- Tokenization 
	- Translate text into numbers
	- Bytepair Encoding Algorithm
	- Python libraries: `SentencePiece`, `Tokenizers` [6] 

#### Model Architecture
			
- **Transformers**
	- Neural network archtecture that uses attention mechanisms
	- Attention mechnism - learns `dependencies` between different elements of sequence based on position and content
		- e.g. "I hit the baseball with a bat", "I hit the bat with a baseball"
	- 3 Types of Transformers
		- `Encoders only` 
			- translates tokens into a semantically meaningful representation
			- tasks: text classification, generate embeddings, 
		- `Decoders only`
			- similar to encoder but does not allow self-attention with future elements
			- tasks: text generation
		- `Encoders and Decoders` 
			- Combine encoders and decoders together and allow cross-attention
			- Tasks: translation 
	- Other Design choices
		- `Residual connections`: allow intermediate training values to bypass hidden layers
		- `Layer normalization`: re-scaling values between layers based on their mean/variance
		- `Activation functions`: introduce non-linearities into model, ReLU, GeLU
		- `Position embedding` - captures information about token positions
	- How big do I make it
		- model too big or trained too long, it can overfit
		- model too small or not trained long enough, it can underperform
		- 20 tokens per model parameters
		- 100x increase in FLOPs for each 10x increase in model parameters
	
				
####  Training at Scale
			
- Mixed precision training 
	- use both 32-bit and 16-bit floating point data types
- 3D parallelism  (pipeline, model, data)
	- combination of pipeline, model and data parallelism
	- Pipeline Parallelism - distribute transformer layers across multiple GPUs
	- Model Parallelism- decomposes parameter matrix operatioon into multiple matrix multiplies 
		distributed across multiple CPUs
	- Data Parallelism - distributed training data across multiple GPUs
- Zero Redundancy Optimizer (ZeRO) 
	- reduces data redundancy regarding the optimizer state, gradient, or parameter partitioning
- Training Stability
	- Checkpointing 
		- Take a snapshot of model artifacts so training can resume from that point
	- Weight decay 
		- regularization strategy that penalize large parameter values by adding a term to loss function
			or changing the parameter update rule
	- Gradient Clipping 
		- rescale the gradient of the objective of its norm exceeds a pre-specified value
- Hyperparameters
	- Batch size: number of training examples used in one iteration of gradient descent
		- static , typically 16 tokens, (Dynamic) GPT-3 increased from 32k to3.2M
	- Learning rate: controls the step size during the optimization process. 
		- learning rate increases linearly until reaching maximum value and then reduces
			vias a cosine decay until the learning rate is about 10% of its max value	
		
#### Evaluation
		
- Benchmark Datasets (open LLM leaderboard) 
	- Arc, HellaSwag, MMLU, TruthFulQA
	- Multiple-choice Tasks, Arc, HellaSwag, MMLU
	- Open-ended Tasks: TruthfulQA
		- Haman evaluation based ground truth, guideline
		- NLP metrics, quantify completion quality via metrics like Perplexity, BLEU or ROGUE scores
		- Auxiliary Fine-Tuned LLM, use LLM to compare completions to ground truth		

### 5 Steps to create a new AI Model
- Foundation model: more focused, centralized effort to create a base model
- Through fine tuning the base foundation model can be adapted to a specialized model
	- Fine-tuning and adapting base fundation models rapidly speeds up AI model development

#### Stage 1: Prepare the data
- Potentially petabytes of data across dozens of domains
- Data processing, filtering, de-duplication, and privacy redaction
- Output `Base Data Pile`

#### Stage 2: Train the model
- Select the base model tha matches data pile 
- tokenize the data pile, base model works with tokens rather than words, data pile could result in potentially trillions of tokens
- Train the base model on the data

#### Stage 3: Validate the model
- Evaluate the model on a variety of tasks and metrics
- Assess its performance against a set of benchmarks to help define the quality of the model
- Create a model card

#### Stage 4: Tune the model
- Stage 2-3 are performed mostly by Data Scientists, Stage 4 is performed by AI/Appplication Engineers (doesn't have to AI expert)
- Fine-tune the model on a specific task to improve its performance on that task
- engage with the model,  generate prompts that elicit good performance from the model
- much quicker than building a model from scratch

#### Stage 5: Deploy the model
- Deploy the model to production
- Could run as service deployed to a public cloud or embed the model into an application that runs much closer to the edge of network
- Iterates and improves the model, and repeat the process

## References
[1] Building LLMs from the Ground Up: A 3-hour Coding Workshop: https://magazine.sebastianraschka.com/p/building-llms-from-the-ground-up  
[2] YT Building LLMs from the Ground Up - https://www.youtube.com/watch?v=quh7z1q7-uc  
[3] How to Build an LLM from Scratch | An Overview: https://www.youtube.com/watch?v=ZLbVdvOoTKM  
[4] Power consumption when training artificial intelligence (AI) based large language models (LLMs) in 2023: https://www.statista.com/statistics/1384401/energy-use-when-training-llm-models/  
[5] Five Steps to Create a New AI Model: https://www.youtube.com/watch?v=jcgaNrC4ElU. 
[6] Build a tokenizer from scratch: https://huggingface.co/docs/tokenizers/quicktour    
